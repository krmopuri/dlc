---
type: lecture
date: 2023-04-03
title: (dl-16) Self-Attention and Transformers

# optional
# please use /static_files/notes directory to store notes
# thumbnail: /static_files/path/to/image.jpg 

# optional
tldr: "Attention is all we need!"
  
# optional
# set it to true if you dont want this lecture to appear in the updates section
hide_from_announcments: false

# optional
links: 
    #- url: /static_files/sgd_update_rules.gif
    #  name: sgd_update_rules.gif
    #- url: https://colab.research.google.com/drive/1OHmvKM7wINUS9kBDQExY_oDKK4AX-wgN?usp=sharing
    #  name: Sample-sequential-task
    - url: /static_files/presentations/dl-16.pdf
      name: slides
    #- url: /static_files/presentations/lec.zip
    #  name: other
---
**Suggested Readings:**
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention is all we need!](https://arxiv.org/pdf/1706.03762.pdf)
- [Computational complexity discussion](https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model)
- [My slides on self-attention layer for CNNs](https://docs.google.com/presentation/d/15ifYMrvQtDrp20H8JWbP94Tifh8OWHlxXt1hiGfpyh0/edit?usp=sharing)
